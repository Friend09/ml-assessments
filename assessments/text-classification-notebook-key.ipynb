{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Assessment (Key)\n",
    "\n",
    "## Objective\n",
    "Classify customer reviews into positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\n",
    "    \"../ml-assessments/datasets/customer-reviews-dataset.csv\"\n",
    ")\n",
    "\n",
    "# Create binary target variable\n",
    "df[\"sentiment\"] = (df[\"rating\"] >= 4).astype(int)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"review_text\"], df[\"sentiment\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   review_id                                        review_text  rating  \\\n",
      "0          1  This product exceeded my expectations. Great v...       5   \n",
      "1          2  Disappointed with the quality. Not worth the p...       2   \n",
      "2          3  Average product, nothing special but does the ...       3   \n",
      "3          4  Absolutely love it! Would highly recommend to ...       5   \n",
      "4          5  Terrible customer service and the product arri...       1   \n",
      "\n",
      "   sentiment  review_length  \n",
      "0          1             61  \n",
      "1          0             51  \n",
      "2          0             50  \n",
      "3          1             55  \n",
      "4          0             58  \n",
      "sentiment\n",
      "0    0.533333\n",
      "1    0.466667\n",
      "Name: proportion, dtype: float64\n",
      "sentiment\n",
      "0    53.5\n",
      "1    53.0\n",
      "Name: review_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the dataset\n",
    "print(df.head())\n",
    "\n",
    "# Show the distribution of positive and negative reviews\n",
    "print(df[\"sentiment\"].value_counts(normalize=True))\n",
    "\n",
    "# Calculate and display the average length of reviews for each class\n",
    "df[\"review_length\"] = df[\"review_text\"].str.len()\n",
    "print(df.groupby(\"sentiment\")[\"review_length\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nI chose TfidfVectorizer over CountVectorizer because it not only considers the frequency of words\\nbut also their importance in the corpus. This can help in giving less weight to common words\\nthat appear in many documents but may not be as informative for classification.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Explanation\n",
    "\"\"\"\n",
    "I chose TfidfVectorizer over CountVectorizer because it not only considers the frequency of words\n",
    "but also their importance in the corpus. This can help in giving less weight to common words\n",
    "that appear in many documents but may not be as informative for classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI chose Logistic Regression because:\\n1. It's suitable for binary classification problems like sentiment analysis.\\n2. It's relatively simple and interpretable.\\n3. It often performs well on text classification tasks, especially with high-dimensional data.\\n4. It's computationally efficient for both training and prediction.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Choose and train the model\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Explanation\n",
    "\"\"\"\n",
    "I chose Logistic Regression because:\n",
    "1. It's suitable for binary classification problems like sentiment analysis.\n",
    "2. It's relatively simple and interpretable.\n",
    "3. It often performs well on text classification tasks, especially with high-dimensional data.\n",
    "4. It's computationally efficient for both training and prediction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices for X_test and y_test\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n",
      "Review: Fantastic! Exactly what I was looking for.\n",
      "Actual sentiment: 1\n",
      "Predicted sentiment: 0\n",
      "\n",
      "Review: Decent product, but the instructions were confusing.\n",
      "Actual sentiment: 0\n",
      "Predicted sentiment: 0\n",
      "\n",
      "Review: This product exceeded my expectations. Great value for money!\n",
      "Actual sentiment: 1\n",
      "Predicted sentiment: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Calculate and display the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "n = min(\n",
    "    5, len(X_test)\n",
    ")  # Ensure we don't go out of bounds if there are fewer than 5 rows\n",
    "\n",
    "for i in range(n):\n",
    "    print(f\"Review: {X_test.iloc[i]}\")\n",
    "    print(f\"Actual sentiment: {y_test.iloc[i]}\")\n",
    "    print(f\"Predicted sentiment: {y_pred[i]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
